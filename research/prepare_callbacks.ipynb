{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\gupta\\\\Documents\\\\GitHub\\\\Brain_Tumor_Detection\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\gupta\\\\Documents\\\\GitHub\\\\Brain_Tumor_Detection'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareCallbacksConfig:\n",
    "    root_dir: Path\n",
    "    tensorboard_root_log_dir: Path\n",
    "    checkpoint_model_filepath: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN_Classifier.constants import *\n",
    "from CNN_Classifier.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_prepare_callback_config(self) -> PrepareCallbacksConfig:\n",
    "        config = self.config.prepare_callbacks\n",
    "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
    "        create_directories([\n",
    "            Path(model_ckpt_dir),\n",
    "            Path(config.tensorboard_root_log_dir)\n",
    "        ])\n",
    "\n",
    "        prepare_callback_config = PrepareCallbacksConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir),\n",
    "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath)\n",
    "        )\n",
    "\n",
    "        return prepare_callback_config\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareCallback:\n",
    "    def __init__(self, config: PrepareCallbacksConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def _create_tb_callbacks(self):\n",
    "        timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        # Use Pathlib's `/` operator instead of os.path.join\n",
    "        tb_running_log_dir = self.config.tensorboard_root_log_dir / f\"tb_logs_at_{timestamp}\"\n",
    "\n",
    "        # Convert Pathlib Path to string for TensorBoard callback\n",
    "        return tf.keras.callbacks.TensorBoard(log_dir=str(tb_running_log_dir))\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def _create_ckpt_callbacks(self):\n",
    "        return tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=str(self.config.checkpoint_model_filepath),\n",
    "            save_best_only=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_tb_ckpt_callbacks(self):\n",
    "        return [\n",
    "            self._create_tb_callbacks,\n",
    "            self._create_ckpt_callbacks\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-09-05 18:46:31,279: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-09-05 18:46:31,289: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-09-05 18:46:31,294: INFO: common: created directory at: artifacts]\n",
      "[2024-09-05 18:46:31,298: INFO: common: created directory at: artifacts\\prepare_callbacks\\checkpoint_dir]\n",
      "[2024-09-05 18:46:31,302: INFO: common: created directory at: artifacts\\prepare_callbacks\\tensorboard_log_dir]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_callbacks_config = config.get_prepare_callback_config()\n",
    "    prepare_callbacks = PrepareCallback(config=prepare_callbacks_config)\n",
    "    callback_list = prepare_callbacks.get_tb_ckpt_callbacks()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback in Real Life: A Kitchen Timer\n",
    "Imagine you're baking a cake, and you use a kitchen timer. You set the timer for 30 minutes, and once the timer goes off, you check if the cake is done. If it's not done, you may decide to keep baking for another 5 minutes. If it's perfect, you take it out of the oven.\n",
    "\n",
    "In this example, the kitchen timer is a type of callback. It's a helper that:\n",
    "\n",
    "Keeps track of time (like logging the progress).\n",
    "Alerts you when it's time to do something (like saving a model or checking accuracy).\n",
    "Decides what to do next based on the state of the cake (e.g., stop baking, keep baking).\n",
    "Callback in Deep Learning\n",
    "In deep learning, callbacks work similarly to the kitchen timer. They automatically do something for you during training at specific moments without you having to intervene manually.\n",
    "\n",
    "Here’s a simple breakdown:\n",
    "\n",
    "Logging Callback (TensorBoard):\n",
    "\n",
    "Example: While the model is training, a callback can log the accuracy and loss values every epoch (round of training). This is like writing down how well the cake is rising every 5 minutes to track the progress.\n",
    "Why it's useful: You can later look at these logs (using TensorBoard) to see how the model is performing and decide if you need to change anything.\n",
    "Model Checkpoint Callback (ModelCheckpoint):\n",
    "\n",
    "Example: Let's say you're training the model for 50 epochs (rounds). Every time the model improves (e.g., higher accuracy or lower loss), the callback saves the model. This is like setting an automatic rule: \"If the cake smells better, take a snapshot of it.\"\n",
    "Why it's useful: You don’t have to manually check the model after each epoch and decide when to save it. The callback saves the best version for you, so you don’t lose progress if something goes wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example of Callbacks\n",
    "Let’s say you’re training a model to recognize handwritten digits (like the famous MNIST dataset). You want to do three things:\n",
    "\n",
    "Track training progress using logs (to see if the model is improving).\n",
    "Save the best model during training (so you don’t lose the best version).\n",
    "Stop training early if the model stops improving (to save time and avoid overfitting).\n",
    "Here’s how callbacks handle that:\n",
    "\n",
    "Without Callbacks:\n",
    "You'd have to manually check the training process after every epoch to log the accuracy and loss.\n",
    "You'd need to save the model manually if you notice that it has improved.\n",
    "You’d have to stop training manually if you see it’s not improving.\n",
    "With Callbacks:\n",
    "Logging Callback: Every epoch, the model automatically logs accuracy and loss for you to check later in TensorBoard.\n",
    "Model Checkpoint Callback: The model automatically saves itself every time it performs better than before (based on a condition you define).\n",
    "Early Stopping Callback: The model can automatically stop training early if it sees that the validation accuracy isn’t improving after a few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Model Checkpoint Works:\n",
    "\n",
    "1. Checkpoint Based on Performance Improvement:\n",
    "\n",
    "If you set save_best_only=True in the ModelCheckpoint callback, the model will only save when its performance improves based on a specific metric (e.g., validation accuracy or validation loss).\n",
    "Suppose you're training for 50 epochs:\n",
    "At epoch 10, if the validation accuracy improves, the model is saved.\n",
    "At epoch 20, if the model performs better than it did at epoch 10, the previous checkpoint (from epoch 10) is replaced with the updated, better model.\n",
    "At epoch 30, if the model’s performance worsens (e.g., higher loss, lower accuracy), the model from epoch 20 is kept, and the model isn't saved again.\n",
    "\n",
    "2. Checkpoint Based on Performance at Each Epoch:\n",
    "\n",
    "The ModelCheckpoint checks the model’s performance after every epoch. However, it will only save the model if it performs better than the previous best performance (when save_best_only=True).\n",
    "If the model’s performance worsens after a certain epoch (higher loss, lower accuracy), the checkpoint will not save the model.\n",
    "If save_best_only=False, it will save the model after every epoch, regardless of whether the performance improves or not.\n",
    "\n",
    "### So, What's Right?\n",
    "\n",
    "Option 2 is closer to the actual behavior:\n",
    "\n",
    "The ModelCheckpoint callback checks the model’s performance after every epoch and saves the model if the performance improves based on a specified metric (like accuracy or loss).\n",
    "If the model starts performing worse in later epochs, it won’t save the model, keeping the best version saved so far.\n",
    "How Does ModelCheckpoint Know When the Model Is Performing Best?\n",
    "The ModelCheckpoint knows the model is performing better by tracking a specific metric over the epochs. You can define which metric to monitor when creating the checkpoint callback, such as:\n",
    "\n",
    "Validation Loss: If the validation loss decreases, the model is considered to be improving.\n",
    "Validation Accuracy: If the validation accuracy increases, the model is considered to be improving.\n",
    "You specify which metric to track using the monitor parameter in the ModelCheckpoint callback. Commonly used metrics are:\n",
    "\n",
    "val_loss: Validation loss\n",
    "val_accuracy: Validation accuracy\n",
    "\n",
    "Example:\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "******* Create a ModelCheckpoint callback\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "\n",
    "    filepath='best_model.h5',  # Where to save the model\n",
    "\n",
    "    monitor='val_accuracy',    # Metric to monitor\n",
    "\n",
    "    save_best_only=True,       # Save only if the model improves\n",
    "\n",
    "    mode='max'                 # 'max' because higher accuracy is better\n",
    "\n",
    ")\n",
    "\n",
    "********* Train the model using the callback\n",
    "\n",
    "history = model.fit(\n",
    "\n",
    "    X_train, y_train,\n",
    "\n",
    "    epochs=50,\n",
    "\n",
    "    validation_data=(X_val, y_val),\n",
    "\n",
    "    callbacks=[checkpoint_cb]  # Include the callback in training\n",
    "    \n",
    ")\n",
    "\n",
    "### Key Points:\n",
    "\n",
    "The checkpoint is saved every time the model performs better based on the monitored metric (val_accuracy or val_loss), which is checked after every epoch.\n",
    "If the model performs worse in subsequent epochs, the previously saved best model will not be replaced.\n",
    "The model is saved in the location specified by the filepath (e.g., 'best_model.h5').\n",
    "mode='max' or mode='min':\n",
    "Use mode='max' if you’re monitoring accuracy or other metrics where higher is better.\n",
    "Use mode='min' if you’re monitoring loss, where lower is better.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
